{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d6b200f-5e10-4baf-8292-1b5279ea46e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from typing import Dict, List, Generator, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr\n",
    "\n",
    "# Tool: DuckDuckGo Search\n",
    "try:\n",
    "    from duckduckgo_search import DDGS\n",
    "    _HAS_DDGS = True\n",
    "except Exception:\n",
    "    _HAS_DDGS = False\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e948b9de-1340-4407-9c56-734613577ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Message helpers (OpenAI-style)\n",
    "# -------------------------\n",
    "def gradio_messages_to_oai(messages: List[Dict], system_prompt: Optional[str]) -> List[Dict]:\n",
    "    \"\"\"Convert Gradio type='messages' into OpenAI-style list and inject optional system prompt.\"\"\"\n",
    "    out = []\n",
    "    if system_prompt:\n",
    "        out.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    for m in messages:\n",
    "        role = m[\"role\"]\n",
    "        content = m.get(\"content\", \"\")\n",
    "        if not content:\n",
    "            continue\n",
    "        # Gradio uses 'assistant'/'user' roles; keep as-is\n",
    "        out.append({\"role\": role, \"content\": content})\n",
    "    return out\n",
    "\n",
    "def add_tool_context(messages: List[Dict], tool_block: str) -> List[Dict]:\n",
    "    \"\"\"Append tool context as a system message.\"\"\"\n",
    "    if not tool_block:\n",
    "        return messages\n",
    "    return messages + [{\"role\": \"system\", \"content\": tool_block}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d13895f-5e0b-4d4f-96d5-f96d2843f360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Web Search Tool\n",
    "# -------------------------\n",
    "def run_web_search(query: str, k: int = 5) -> str:\n",
    "    if not _HAS_DDGS:\n",
    "        return \"Web search unavailable: duckduckgo_search not installed.\"\n",
    "    if not query or not query.strip():\n",
    "        return \"\"\n",
    "    try:\n",
    "        results = []\n",
    "        with DDGS() as ddgs:\n",
    "            for r in ddgs.text(query, max_results=k):\n",
    "                # r: {'title':..., 'href':..., 'body':...}\n",
    "                results.append(r)\n",
    "        if not results:\n",
    "            return \"No results.\"\n",
    "        # Format compactly for model context\n",
    "        lines = []\n",
    "        for i, r in enumerate(results, 1):\n",
    "            title = r.get(\"title\", \"\").strip()\n",
    "            href = r.get(\"href\", \"\").strip()\n",
    "            body = r.get(\"body\", \"\").strip()\n",
    "            lines.append(f\"{i}. {title}\\nURL: {href}\\nSnippet: {body}\")\n",
    "        return \"WEB_SEARCH_RESULTS\\n\" + \"\\n\\n\".join(lines)\n",
    "    except Exception as e:\n",
    "        return f\"Web search error: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4cbcf9bd-5272-474c-bb6a-1d9c6367d3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Providers\n",
    "# -------------------------\n",
    "@dataclass\n",
    "class ChatConfig:\n",
    "    provider: str\n",
    "    model: str\n",
    "    temperature: float = 0.2\n",
    "\n",
    "class BaseChat:\n",
    "    def stream_chat(self, messages: List[Dict], cfg: ChatConfig) -> Generator[str, None, None]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "class OpenAIChat(BaseChat):\n",
    "    def stream_chat(self, messages: List[Dict], cfg: ChatConfig):\n",
    "        from openai import OpenAI\n",
    "        api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        if not api_key:\n",
    "            yield \"[OpenAI] Missing OPENAI_API_KEY.\"\n",
    "            return\n",
    "        client = OpenAI(api_key=api_key)\n",
    "        try:\n",
    "            stream = client.chat.completions.create(\n",
    "                model=cfg.model,\n",
    "                messages=messages,\n",
    "                temperature=cfg.temperature,\n",
    "                stream=True,\n",
    "            )\n",
    "            for chunk in stream:\n",
    "                delta = chunk.choices[0].delta\n",
    "                if delta and delta.content:\n",
    "                    yield delta.content\n",
    "        except Exception as e:\n",
    "            yield f\"[OpenAI] Error: {e}\"\n",
    "\n",
    "class AnthropicChat(BaseChat):\n",
    "    def stream_chat(self, messages: List[Dict], cfg: ChatConfig):\n",
    "        import anthropic\n",
    "        api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "        if not api_key:\n",
    "            yield \"[Anthropic] Missing ANTHROPIC_API_KEY.\"\n",
    "            return\n",
    "        client = anthropic.Anthropic(api_key=api_key)\n",
    "        # Convert OpenAI-style messages to Anthropic format\n",
    "        sys_prompt = \"\"\n",
    "        user_turns = []\n",
    "        for m in messages:\n",
    "            if m[\"role\"] == \"system\":\n",
    "                sys_prompt += (m[\"content\"] + \"\\n\")\n",
    "            elif m[\"role\"] == \"user\":\n",
    "                user_turns.append({\"role\": \"user\", \"content\": m[\"content\"]})\n",
    "            elif m[\"role\"] == \"assistant\":\n",
    "                user_turns.append({\"role\": \"assistant\", \"content\": m[\"content\"]})\n",
    "        try:\n",
    "            with client.messages.stream(\n",
    "                model=cfg.model,\n",
    "                max_tokens=2048,\n",
    "                temperature=cfg.temperature,\n",
    "                system=sys_prompt if sys_prompt else None,\n",
    "                messages=user_turns,\n",
    "            ) as stream:\n",
    "                for text in stream.text_stream:\n",
    "                    if text:\n",
    "                        yield text\n",
    "        except Exception as e:\n",
    "            yield f\"[Anthropic] Error: {e}\"\n",
    "\n",
    "class GoogleChat(BaseChat):\n",
    "    def stream_chat(self, messages: List[Dict], cfg: ChatConfig):\n",
    "        import google.generativeai as genai\n",
    "        api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "        if not api_key:\n",
    "            yield \"[Google] Missing GOOGLE_API_KEY.\"\n",
    "            return\n",
    "        genai.configure(api_key=api_key)\n",
    "        # Split out system instruction and history\n",
    "        system_instruction = \"\\n\".join([m[\"content\"] for m in messages if m[\"role\"] == \"system\"])\n",
    "        dialog = []\n",
    "        for m in messages:\n",
    "            if m[\"role\"] == \"user\":\n",
    "                dialog.append({\"role\": \"user\", \"parts\": [m[\"content\"]]})\n",
    "            elif m[\"role\"] == \"assistant\":\n",
    "                dialog.append({\"role\": \"model\", \"parts\": [m[\"content\"]]})\n",
    "        try:\n",
    "            model = genai.GenerativeModel(model_name=cfg.model, system_instruction=system_instruction or None)\n",
    "            resp = model.generate_content(dialog if dialog else [{\"role\":\"user\",\"parts\":[\"Hello\"]}],\n",
    "                                          generation_config={\"temperature\": cfg.temperature},\n",
    "                                          stream=True)\n",
    "            for r in resp:\n",
    "                if hasattr(r, \"text\") and r.text:\n",
    "                    yield r.text\n",
    "        except Exception as e:\n",
    "            yield f\"[Google] Error: {e}\"\n",
    "\n",
    "class OllamaChat(BaseChat):\n",
    "    def stream_chat(self, messages: List[Dict], cfg: ChatConfig):\n",
    "        try:\n",
    "            import ollama\n",
    "        except ImportError:\n",
    "            yield \"[Ollama] Error: ollama package not installed. Run: pip install ollama\"\n",
    "            return\n",
    "            \n",
    "        try:\n",
    "            # First try non-streaming to debug\n",
    "            response = ollama.chat(\n",
    "                model=cfg.model,\n",
    "                messages=messages,\n",
    "                stream=False,\n",
    "                options={'temperature': cfg.temperature}\n",
    "            )\n",
    "            \n",
    "            # Get the full response content\n",
    "            if response and 'message' in response:\n",
    "                content = response['message'].get('content', '')\n",
    "                if content:\n",
    "                    # Yield the full content at once (simpler than streaming)\n",
    "                    yield content\n",
    "                else:\n",
    "                    yield f\"[Ollama] Empty response from model '{cfg.model}'\"\n",
    "            else:\n",
    "                yield f\"[Ollama] Invalid response format from model '{cfg.model}'\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            yield f\"[Ollama] Error: {e}\"\n",
    "\n",
    "\n",
    "PROVIDER_IMPLS = {\n",
    "    \"openai\": OpenAIChat(),\n",
    "    \"anthropic\": AnthropicChat(),\n",
    "    \"google\": GoogleChat(),\n",
    "    \"ollama\": OllamaChat(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a99fea0f-be58-422b-a6e2-bfee0b1d7253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Defaults\n",
    "# -------------------------\n",
    "DEFAULT_SYSTEM = (\n",
    "    \"You are a senior NLP/ML engineer. Answer technical questions concisely. \"\n",
    "    \"Show minimal code when useful. Cite sources only if provided in context. \"\n",
    "    \"When users provide error messages, diagnose and propose precise fixes.\"\n",
    ")\n",
    "\n",
    "OPENAI_DEFAULT = \"gpt-4o-mini\"\n",
    "ANTHROPIC_DEFAULT = \"claude-sonnet-4-0\"\n",
    "GOOGLE_DEFAULT = \"gemini-2.0-flash\"\n",
    "OLLAMA_DEFAULT = \"llama3.2\"  # or another local model you have pulled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0edeb0b4-5fd3-411c-90fc-efa59d6b060a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7870\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7870/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -------------------------\n",
    "# Chat function for Gradio\n",
    "# -------------------------\n",
    "def chat_fn(user_message, history, provider, model, temperature, use_web_tool, web_k, system_prompt):\n",
    "    \"\"\"Gradio callback. Must yield text chunks for streaming.\"\"\"\n",
    "    # 1) Build base messages from history + current user message\n",
    "    if history is None:\n",
    "        history = []\n",
    "    # history is already [{\"role\":..., \"content\":...}] because we set type='messages'\n",
    "    base = gradio_messages_to_oai(history, system_prompt or DEFAULT_SYSTEM)\n",
    "    base.append({\"role\": \"user\", \"content\": user_message})\n",
    "\n",
    "    # 2) Optional web tool\n",
    "    tool_block = \"\"\n",
    "    if use_web_tool:\n",
    "        tool_block = run_web_search(user_message, k=web_k)\n",
    "        if tool_block:\n",
    "            base = add_tool_context(base, tool_block)\n",
    "\n",
    "    # 3) Dispatch to provider\n",
    "    provider_key = (provider or \"\").strip().lower()\n",
    "    impl = PROVIDER_IMPLS.get(provider_key)\n",
    "    if not impl:\n",
    "        yield f\"[Error] Unsupported provider: {provider}\"\n",
    "        return\n",
    "\n",
    "    cfg = ChatConfig(provider=provider_key, model=model, temperature=temperature)\n",
    "\n",
    "    # 4) If tool used, show a short header first (so users see the tool at work), then stream the model\n",
    "    if tool_block:\n",
    "        # Short preface, then a horizontal rule\n",
    "        summary = \"Using web search context (top results included in system context).\\n---\\n\"\n",
    "        yield summary\n",
    "\n",
    "    # 5) Stream\n",
    "    response = \"\"\n",
    "    for chunk in impl.stream_chat(base, cfg):\n",
    "        response += chunk or \"\"\n",
    "        yield response\n",
    "\n",
    "        if not response:\n",
    "            yield \"[No response]\"\n",
    "\n",
    "# -------------------------\n",
    "# UI\n",
    "# -------------------------\n",
    "with gr.Blocks(title=\"Technical Q/A Prototype\") as demo:\n",
    "    gr.Markdown(\"### Technical Q/A Prototype — streaming, system prompt, model switch, simple web tool\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1, min_width=320):\n",
    "            provider = gr.Dropdown(\n",
    "                label=\"Provider\",\n",
    "                choices=[\"openai\", \"anthropic\", \"google\", \"ollama\"],\n",
    "                value=\"openai\",\n",
    "            )\n",
    "            model = gr.Textbox(\n",
    "                label=\"Model name\",\n",
    "                value=OPENAI_DEFAULT,\n",
    "                placeholder=\"e.g., gpt-4o-mini / claude-3-5-sonnet-latest / gemini-1.5-pro / llama3.1\",\n",
    "            )\n",
    "            temperature = gr.Slider(0.0, 1.0, value=0.2, step=0.05, label=\"Temperature\")\n",
    "            use_web_tool = gr.Checkbox(label=\"Use web search tool (DuckDuckGo)\", value=False)\n",
    "            web_k = gr.Slider(1, 10, value=5, step=1, label=\"Web results (k)\")\n",
    "            system_prompt = gr.Textbox(\n",
    "                label=\"System prompt\",\n",
    "                value=DEFAULT_SYSTEM,\n",
    "                lines=4,\n",
    "            )\n",
    "            gr.Markdown(\n",
    "                \"API keys via `.env`: `OPENAI_API_KEY`, `ANTHROPIC_API_KEY`, `GOOGLE_API_KEY`. \"\n",
    "                \"Ollama requires a local server.\"\n",
    "            )\n",
    "\n",
    "        with gr.Column(scale=2):\n",
    "            chat = gr.ChatInterface(\n",
    "                fn=chat_fn,\n",
    "                type=\"messages\",  # important for future gradio compatibility\n",
    "                additional_inputs=[provider, model, temperature, use_web_tool, web_k, system_prompt],\n",
    "                textbox=gr.Textbox(placeholder=\"Ask a technical question…\", lines=3, autofocus=True, submit_btn=True),\n",
    "                cache_examples=False,\n",
    "                \n",
    "            )\n",
    "\n",
    "    # Sensible defaults on provider change\n",
    "    def _set_default_model(p):\n",
    "        p = (p or \"\").lower()\n",
    "        if p == \"openai\":\n",
    "            return OPENAI_DEFAULT\n",
    "        if p == \"anthropic\":\n",
    "            return ANTHROPIC_DEFAULT\n",
    "        if p == \"google\":\n",
    "            return GOOGLE_DEFAULT\n",
    "        if p == \"ollama\":\n",
    "            return OLLAMA_DEFAULT\n",
    "        return \"\"\n",
    "    provider.change(_set_default_model, inputs=provider, outputs=model)\n",
    "\n",
    "demo.launch(inline=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c239f79d-480d-469a-8f98-da12ecf8e8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                ID              SIZE      MODIFIED     \n",
      "deepseek-r1:1.5b    e0979632db5a    1.1 GB    26 hours ago    \n",
      "llama3.2:latest     a80c4f17acd5    2.0 GB    26 hours ago    \n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
